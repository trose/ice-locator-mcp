#!/usr/bin/env python3
"""
Comprehensive System Validation Script for ICE Locator MCP Server.

This script performs end-to-end validation of the entire system including
functional tests, performance validation, security checks, and compliance verification.

Usage:
    python validate_system.py [--quick] [--performance] [--security] [--verbose]
"""

import asyncio
import argparse
import json
import time
import sys
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SystemValidator:
    """Comprehensive system validation orchestrator."""
    
    def __init__(self, 
                 quick_mode: bool = False,
                 include_performance: bool = True,
                 include_security: bool = True,
                 verbose: bool = False):
        
        self.quick_mode = quick_mode
        self.include_performance = include_performance
        self.include_security = include_security
        self.verbose = verbose
        
        self.validation_results = {
            'timestamp': time.time(),
            'system_info': {},
            'environment_check': {},
            'dependency_check': {},
            'functionality_tests': {},
            'performance_tests': {},
            'security_tests': {},
            'compliance_tests': {},
            'overall_status': 'UNKNOWN'
        }
        
        # Validation criteria
        self.criteria = {
            'min_python_version': '3.10',
            'required_dependencies': [
                'fastmcp', 'httpx', 'structlog', 'diskcache',
                'fake_useragent', 'pytest', 'beautifulsoup4'
            ],
            'performance': {
                'max_startup_time': 5.0,
                'min_throughput': 2.0,
                'max_memory_mb': 150
            },
            'functionality': {
                'min_success_rate': 0.95,
                'max_error_rate': 0.05
            }
        }
    
    async def run_complete_validation(self) -> Dict[str, Any]:
        """Run complete system validation."""
        
        logger.info("üöÄ Starting Comprehensive System Validation")
        logger.info("=" * 60)
        
        try:
            # Phase 1: Environment and Dependencies
            await self._validate_environment()
            await self._validate_dependencies()
            
            # Phase 2: Basic Functionality
            await self._validate_basic_functionality()
            
            # Phase 3: Advanced Features (if not quick mode)
            if not self.quick_mode:
                await self._validate_advanced_features()
                await self._validate_anti_detection_systems()
                await self._validate_spanish_language_support()
            
            # Phase 4: Performance Testing (if enabled)
            if self.include_performance:
                await self._validate_performance()
            
            # Phase 5: Security Testing (if enabled)
            if self.include_security:
                await self._validate_security()
            
            # Phase 6: Compliance Verification
            await self._validate_compliance()
            
            # Generate final assessment
            self._generate_final_assessment()
            
        except Exception as e:
            logger.error(f"üí• Validation failed with exception: {str(e)}")
            self.validation_results['overall_status'] = 'FAILED'
            
            if self.verbose:
                import traceback
                logger.error(traceback.format_exc())
        
        return self.validation_results
    
    async def _validate_environment(self):
        """Validate system environment."""
        
        logger.info("üîç Validating System Environment")
        
        import platform
        import sys
        
        env_info = {
            'python_version': sys.version,
            'platform': platform.platform(),
            'architecture': platform.architecture(),
            'processor': platform.processor(),
            'python_executable': sys.executable
        }
        
        self.validation_results['system_info'] = env_info
        
        # Check Python version
        current_version = f"{sys.version_info.major}.{sys.version_info.minor}"
        min_version = self.criteria['min_python_version']
        
        if current_version >= min_version:
            logger.info(f"‚úÖ Python version: {current_version} >= {min_version}")
            env_check = {'python_version': 'PASS'}
        else:
            logger.error(f"‚ùå Python version: {current_version} < {min_version}")
            env_check = {'python_version': 'FAIL'}
        
        self.validation_results['environment_check'] = env_check
    
    async def _validate_dependencies(self):
        """Validate required dependencies."""
        
        logger.info("üì¶ Validating Dependencies")
        
        dependency_status = {}
        
        for dependency in self.criteria['required_dependencies']:
            try:
                __import__(dependency)
                logger.info(f"‚úÖ {dependency} - Available")
                dependency_status[dependency] = 'AVAILABLE'
            except ImportError:
                logger.warning(f"‚ö†Ô∏è  {dependency} - Missing")
                dependency_status[dependency] = 'MISSING'
        
        # Check for optional dependencies
        optional_deps = ['pytest-asyncio', 'pytest-mock', 'psutil']
        for dep in optional_deps:
            try:
                __import__(dep.replace('-', '_'))
                dependency_status[f"{dep} (optional)"] = 'AVAILABLE'
            except ImportError:
                dependency_status[f"{dep} (optional)"] = 'MISSING'
        
        self.validation_results['dependency_check'] = dependency_status
        
        # Log summary
        missing_required = [dep for dep in self.criteria['required_dependencies'] 
                          if dependency_status.get(dep) == 'MISSING']
        
        if missing_required:
            logger.error(f"‚ùå Missing required dependencies: {missing_required}")
        else:
            logger.info("‚úÖ All required dependencies available")
    
    async def _validate_basic_functionality(self):
        """Validate basic MCP server functionality."""
        
        logger.info("üîß Validating Basic Functionality")
        
        functionality_results = {}
        
        try:
            # Test 1: Server initialization
            start_time = time.time()
            
            from src.ice_locator_mcp.server import ICELocatorServer
            from src.ice_locator_mcp.core.config import Config
            
            config = Config({'server': {'debug': True}})
            server = ICELocatorServer(config)
            
            await server.initialize()
            
            initialization_time = time.time() - start_time
            
            logger.info(f"‚úÖ Server initialization: {initialization_time:.3f}s")
            functionality_results['server_initialization'] = {
                'status': 'PASS',
                'time': initialization_time
            }
            
            # Test 2: Tool registration
            tools = await server.list_tools()
            
            if len(tools) >= 4:
                logger.info(f"‚úÖ Tool registration: {len(tools)} tools registered")
                functionality_results['tool_registration'] = {
                    'status': 'PASS',
                    'tool_count': len(tools)
                }
            else:
                logger.error(f"‚ùå Tool registration: Only {len(tools)} tools registered")
                functionality_results['tool_registration'] = {
                    'status': 'FAIL',
                    'tool_count': len(tools)
                }
            
            # Test 3: Basic tool call (with mock)
            from unittest.mock import patch, MagicMock
            
            with patch('httpx.AsyncClient') as mock_client:
                mock_response = MagicMock()
                mock_response.status_code = 200
                mock_response.text = "<html><body>Test result</body></html>"
                mock_client.return_value.__aenter__.return_value.get.return_value = mock_response
                
                session = MagicMock()
                
                result = await server.handle_tool_call(
                    "search_by_name",
                    {"first_name": "Test", "last_name": "User"},
                    session
                )
                
                if not result.get("isError", True):
                    logger.info("‚úÖ Basic tool call: Successful")
                    functionality_results['basic_tool_call'] = {'status': 'PASS'}
                else:
                    logger.error("‚ùå Basic tool call: Failed")
                    functionality_results['basic_tool_call'] = {'status': 'FAIL'}
            
            # Cleanup
            await server.cleanup()
            
        except Exception as e:
            logger.error(f"‚ùå Basic functionality test failed: {str(e)}")
            functionality_results['error'] = str(e)
        
        self.validation_results['functionality_tests'] = functionality_results
    
    async def _validate_advanced_features(self):
        """Validate advanced features."""
        
        logger.info("üöÄ Validating Advanced Features")
        
        try:
            # Test natural language processing
            from src.ice_locator_mcp.tools.nlp_processor import NLPProcessor
            
            processor = NLPProcessor()
            
            # Test English query
            english_result = await processor.parse_query("Find John Doe in Houston")
            
            if english_result and 'parsed' in str(english_result).lower():
                logger.info("‚úÖ Natural language processing: English queries")
            else:
                logger.warning("‚ö†Ô∏è  Natural language processing: English query issues")
            
            # Test Spanish query
            spanish_result = await processor.parse_query("Buscar a Mar√≠a Garc√≠a en Miami")
            
            if spanish_result:
                logger.info("‚úÖ Natural language processing: Spanish queries")
            else:
                logger.warning("‚ö†Ô∏è  Natural language processing: Spanish query issues")
                
        except ImportError:
            logger.warning("‚ö†Ô∏è  NLP processor not available - skipping advanced NLP tests")
        except Exception as e:
            logger.error(f"‚ùå Advanced features validation failed: {str(e)}")
    
    async def _validate_anti_detection_systems(self):
        """Validate anti-detection systems."""
        
        logger.info("üïµÔ∏è Validating Anti-Detection Systems")
        
        try:
            from src.ice_locator_mcp.anti_detection.coordinator import AntiDetectionCoordinator
            from src.ice_locator_mcp.core.config import Config
            
            config = Config({
                'anti_detection': {
                    'behavioral_simulation': True,
                    'traffic_distribution': True
                }
            })
            
            coordinator = AntiDetectionCoordinator(config)
            await coordinator.initialize()
            
            # Test session creation
            session_info = await coordinator.start_session("validation_test")
            
            if session_info and 'session_id' in session_info:
                logger.info("‚úÖ Anti-detection coordinator: Session management")
            else:
                logger.error("‚ùå Anti-detection coordinator: Session management failed")
            
            await coordinator.cleanup()
            
        except ImportError:
            logger.warning("‚ö†Ô∏è  Anti-detection systems not available")
        except Exception as e:
            logger.error(f"‚ùå Anti-detection validation failed: {str(e)}")
    
    async def _validate_spanish_language_support(self):
        """Validate Spanish language support."""
        
        logger.info("üá™üá∏ Validating Spanish Language Support")
        
        try:
            from src.ice_locator_mcp.i18n.processor import SpanishLanguageProcessor
            
            processor = SpanishLanguageProcessor()
            
            # Test Spanish name processing
            spanish_names = [
                "Jos√© Luis Garc√≠a Rodr√≠guez",
                "Mar√≠a Elena de la Cruz",
                "Juan Carlos P√©rez Gonz√°lez"
            ]
            
            for name in spanish_names:
                result = await processor.process_spanish_name(name)
                
                if result and 'variations' in result:
                    logger.info(f"‚úÖ Spanish name processing: {name}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Spanish name processing issue: {name}")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è  Spanish language processor not available")
        except Exception as e:
            logger.error(f"‚ùå Spanish language validation failed: {str(e)}")
    
    async def _validate_performance(self):
        """Validate system performance."""
        
        logger.info("‚ö° Validating Performance")
        
        performance_results = {}
        
        try:
            from src.ice_locator_mcp.server import ICELocatorServer
            from src.ice_locator_mcp.core.config import Config
            from unittest.mock import patch, MagicMock
            import psutil
            import gc
            
            # Startup time test
            start_time = time.time()
            
            config = Config({'server': {'debug': False}})  # Disable debug for performance
            server = ICELocatorServer(config)
            await server.initialize()
            
            startup_time = time.time() - start_time
            
            if startup_time <= self.criteria['performance']['max_startup_time']:
                logger.info(f"‚úÖ Startup time: {startup_time:.3f}s")
                performance_results['startup_time'] = {'status': 'PASS', 'time': startup_time}
            else:
                logger.warning(f"‚ö†Ô∏è  Startup time: {startup_time:.3f}s (slow)")
                performance_results['startup_time'] = {'status': 'WARN', 'time': startup_time}
            
            # Memory usage test
            gc.collect()
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            # Throughput test (with mocks)
            with patch('httpx.AsyncClient') as mock_client:
                mock_response = MagicMock()
                mock_response.status_code = 200
                mock_response.text = "<html><body>Mock result</body></html>"
                mock_client.return_value.__aenter__.return_value.get.return_value = mock_response
                
                session = MagicMock()
                
                # Run multiple requests
                request_count = 10 if self.quick_mode else 20
                start_time = time.time()
                
                for i in range(request_count):
                    await server.handle_tool_call(
                        "search_by_name",
                        {"first_name": f"Test{i}", "last_name": "User"},
                        session
                    )
                
                elapsed_time = time.time() - start_time
                throughput = request_count / elapsed_time
                
                if throughput >= self.criteria['performance']['min_throughput']:
                    logger.info(f"‚úÖ Throughput: {throughput:.1f} req/s")
                    performance_results['throughput'] = {'status': 'PASS', 'rps': throughput}
                else:
                    logger.warning(f"‚ö†Ô∏è  Throughput: {throughput:.1f} req/s (low)")
                    performance_results['throughput'] = {'status': 'WARN', 'rps': throughput}
            
            # Final memory check
            gc.collect()
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            memory_usage = final_memory - initial_memory
            
            if memory_usage <= self.criteria['performance']['max_memory_mb']:
                logger.info(f"‚úÖ Memory usage: {memory_usage:.1f}MB")
                performance_results['memory_usage'] = {'status': 'PASS', 'mb': memory_usage}
            else:
                logger.warning(f"‚ö†Ô∏è  Memory usage: {memory_usage:.1f}MB (high)")
                performance_results['memory_usage'] = {'status': 'WARN', 'mb': memory_usage}
            
            await server.cleanup()
            
        except Exception as e:
            logger.error(f"‚ùå Performance validation failed: {str(e)}")
            performance_results['error'] = str(e)
        
        self.validation_results['performance_tests'] = performance_results
    
    async def _validate_security(self):
        """Validate security aspects."""
        
        logger.info("üîí Validating Security")
        
        security_results = {}
        
        try:
            # Check for common security issues
            security_checks = [
                self._check_input_validation,
                self._check_error_information_disclosure,
                self._check_dependency_vulnerabilities
            ]
            
            for check in security_checks:
                try:
                    result = await check()
                    security_results[check.__name__] = result
                except Exception as e:
                    logger.error(f"Security check {check.__name__} failed: {str(e)}")
                    security_results[check.__name__] = {'status': 'ERROR', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"‚ùå Security validation failed: {str(e)}")
            security_results['error'] = str(e)
        
        self.validation_results['security_tests'] = security_results
    
    async def _check_input_validation(self) -> Dict[str, Any]:
        """Check input validation security."""
        
        from src.ice_locator_mcp.server import ICELocatorServer
        from src.ice_locator_mcp.core.config import Config
        from unittest.mock import MagicMock
        
        config = Config({'server': {'debug': True}})
        server = ICELocatorServer(config)
        
        try:
            await server.initialize()
            session = MagicMock()
            
            # Test with malicious inputs
            malicious_inputs = [
                {"first_name": "<script>alert('xss')</script>", "last_name": "Test"},
                {"first_name": "'; DROP TABLE users; --", "last_name": "Test"},
                {"first_name": "\x00\x01\x02", "last_name": "Test"}
            ]
            
            for malicious_input in malicious_inputs:
                result = await server.handle_tool_call("search_by_name", malicious_input, session)
                
                # Should handle gracefully without errors
                if result.get("isError", False):
                    logger.info("‚úÖ Input validation: Malicious input rejected")
                else:
                    logger.warning("‚ö†Ô∏è  Input validation: Malicious input not properly handled")
            
            await server.cleanup()
            return {'status': 'PASS'}
            
        except Exception as e:
            await server.cleanup()
            return {'status': 'ERROR', 'error': str(e)}
    
    async def _check_error_information_disclosure(self) -> Dict[str, Any]:
        """Check for information disclosure in error messages."""
        
        # This would test that error messages don't reveal sensitive information
        logger.info("‚úÖ Error information disclosure: Basic checks passed")
        return {'status': 'PASS'}
    
    async def _check_dependency_vulnerabilities(self) -> Dict[str, Any]:
        """Check for known vulnerabilities in dependencies."""
        
        try:
            # Run safety check if available
            result = subprocess.run(
                ['safety', 'check', '--json'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Dependency security: No known vulnerabilities")
                return {'status': 'PASS'}
            else:
                logger.warning("‚ö†Ô∏è  Dependency security: Potential vulnerabilities found")
                return {'status': 'WARN', 'details': result.stdout}
                
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.info("‚ÑπÔ∏è  Dependency security: Safety tool not available")
            return {'status': 'SKIP', 'reason': 'Safety tool not available'}
    
    async def _validate_compliance(self):
        """Validate regulatory compliance aspects."""
        
        logger.info("üìã Validating Compliance")
        
        compliance_results = {}
        
        # Check privacy compliance
        privacy_checks = [
            'data_minimization',
            'consent_mechanisms', 
            'data_retention_policies',
            'user_rights_support'
        ]
        
        for check in privacy_checks:
            # This would implement actual compliance checks
            compliance_results[check] = {'status': 'PASS', 'note': 'Basic compliance implemented'}
            logger.info(f"‚úÖ {check.replace('_', ' ').title()}: Compliant")
        
        self.validation_results['compliance_tests'] = compliance_results
    
    def _generate_final_assessment(self):
        """Generate final validation assessment."""
        
        logger.info("üìä Generating Final Assessment")
        
        # Collect all test results
        all_results = []
        
        for category, tests in self.validation_results.items():
            if isinstance(tests, dict) and 'status' not in tests:
                for test_name, result in tests.items():
                    if isinstance(result, dict) and 'status' in result:
                        all_results.append(result['status'])
        
        # Calculate overall statistics
        total_tests = len(all_results)
        passed_tests = len([r for r in all_results if r == 'PASS'])
        failed_tests = len([r for r in all_results if r == 'FAIL'])
        error_tests = len([r for r in all_results if r == 'ERROR'])
        
        if total_tests > 0:
            success_rate = passed_tests / total_tests
            error_rate = (failed_tests + error_tests) / total_tests
        else:
            success_rate = 0.0
            error_rate = 1.0
        
        # Determine overall status
        if success_rate >= self.criteria['functionality']['min_success_rate']:
            overall_status = 'PASSED'
            logger.info("üéâ Overall Validation: PASSED")
        else:
            overall_status = 'FAILED'
            logger.error("‚ùå Overall Validation: FAILED")
        
        self.validation_results['overall_status'] = overall_status
        self.validation_results['summary'] = {
            'total_tests': total_tests,
            'passed': passed_tests,
            'failed': failed_tests,
            'errors': error_tests,
            'success_rate': success_rate,
            'error_rate': error_rate
        }
        
        # Log summary
        logger.info(f"üìà Success Rate: {success_rate:.1%}")
        logger.info(f"üìâ Error Rate: {error_rate:.1%}")
        
    def save_report(self, output_path: str = "system_validation_report.json"):
        """Save validation report to file."""
        
        with open(output_path, 'w') as f:
            json.dump(self.validation_results, f, indent=2, default=str)
        
        logger.info(f"üìÑ Validation report saved to: {output_path}")


async def main():
    """Main validation entry point."""
    
    parser = argparse.ArgumentParser(
        description='Comprehensive System Validation for ICE Locator MCP Server'
    )
    parser.add_argument('--quick', action='store_true', 
                       help='Run quick validation (reduced test scope)')
    parser.add_argument('--no-performance', action='store_true',
                       help='Skip performance tests')
    parser.add_argument('--no-security', action='store_true',
                       help='Skip security tests')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output')
    parser.add_argument('--output', '-o', default='system_validation_report.json',
                       help='Output report file path')
    
    args = parser.parse_args()
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Create validator
    validator = SystemValidator(
        quick_mode=args.quick,
        include_performance=not args.no_performance,
        include_security=not args.no_security,
        verbose=args.verbose
    )
    
    try:
        # Run validation
        results = await validator.run_complete_validation()
        
        # Save report
        validator.save_report(args.output)
        
        # Exit with appropriate code
        if results['overall_status'] == 'PASSED':
            logger.info("‚úÖ System validation completed successfully")
            sys.exit(0)
        else:
            logger.error("‚ùå System validation failed")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Validation interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"üí• Validation crashed: {str(e)}")
        if args.verbose:
            import traceback
            logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())