name: Comprehensive Integration Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly integration tests
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suites:
        description: 'Specific test suites to run (comma-separated)'
        required: false
        default: 'all'
      performance_tests:
        description: 'Include performance tests'
        type: boolean
        default: true
      verbose_output:
        description: 'Enable verbose test output'
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  lint-and-format:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install black isort flake8 mypy
          pip install -e .
          
      - name: Code formatting check
        run: |
          black --check --diff src tests
          isort --check-only --diff src tests
          
      - name: Linting
        run: |
          flake8 src tests --max-line-length=100 --ignore=E203,W503
          
      - name: Type checking
        run: |
          mypy src --ignore-missing-imports --no-strict-optional

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-asyncio pytest-mock
          pip install -e .
          
      - name: Run unit tests
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term
          
      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.11'
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests

  integration-tests-basic:
    name: Basic Integration Tests
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock psutil
          pip install -e .
          
      - name: Run basic integration tests
        run: |
          cd tests
          python integration_test_runner.py --suites basic_functionality complete_workflows --verbose
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: basic-integration-results
          path: tests/integration_test_report.json

  integration-tests-advanced:
    name: Advanced Integration Tests
    runs-on: ubuntu-latest
    needs: [integration-tests-basic]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock psutil
          pip install -e .
          
      - name: Run advanced integration tests
        run: |
          cd tests
          python integration_test_runner.py --suites anti_detection_integration spanish_language_support error_handling_resilience --verbose
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: advanced-integration-results
          path: tests/integration_test_report.json

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [integration-tests-basic]
    if: github.event.inputs.performance_tests != 'false'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock psutil statistics
          pip install -e .
          
      - name: Run performance benchmarks
        run: |
          cd tests
          python integration_test_runner.py --suites performance_validation --verbose
          
      - name: Analyze performance results
        run: |
          cd tests
          python -c "
          import json
          with open('integration_test_report.json', 'r') as f:
              report = json.load(f)
          
          # Extract performance metrics
          perf_tests = [r for r in report['test_results'] if 'performance' in r['test_name'].lower()]
          
          if perf_tests:
              avg_time = sum(r['execution_time'] for r in perf_tests) / len(perf_tests)
              print(f'Average performance test time: {avg_time:.3f}s')
              
              # Check for performance regressions
              if avg_time > 5.0:
                  print('WARNING: Performance tests taking longer than expected')
                  exit(1)
          "
          
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results
          path: tests/integration_test_report.json

  client-compatibility-tests:
    name: MCP Client Compatibility Tests
    runs-on: ubuntu-latest
    needs: [integration-tests-basic]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock
          pip install -e .
          
      - name: Install MCP SDK for testing
        run: |
          npm install -g @modelcontextprotocol/sdk
          
      - name: Run client compatibility tests
        run: |
          cd tests
          python integration_test_runner.py --suites client_compatibility --verbose
          
      - name: Upload compatibility results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: client-compatibility-results
          path: tests/integration_test_report.json

  security-tests:
    name: Security and Compliance Tests
    runs-on: ubuntu-latest
    needs: [integration-tests-basic]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
          pip install pytest pytest-asyncio pytest-mock
          pip install -e .
          
      - name: Run security scans
        run: |
          # Check for security vulnerabilities in dependencies
          safety check
          
          # Static security analysis
          bandit -r src/ -f json -o bandit-report.json || true
          
      - name: Run security-focused tests
        run: |
          # Run tests that focus on security aspects
          pytest tests/unit/test_security.py -v || true
          pytest tests/unit/test_privacy.py -v || true
          
      - name: Upload security results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-test-results
          path: bandit-report.json

  cross-platform-tests:
    name: Cross-Platform Tests
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    runs-on: ${{ matrix.os }}
    needs: [unit-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock
          pip install -e .
          
      - name: Run cross-platform tests
        run: |
          # Run a subset of tests to ensure basic functionality across platforms
          pytest tests/unit/test_core.py -v
          pytest tests/unit/test_tools.py -v
          
      - name: Test MCP server startup
        run: |
          # Test that the server can start on each platform
          python -c "
          import asyncio
          from src.ice_locator_mcp.server import ICELocatorServer
          from src.ice_locator_mcp.core.config import Config
          
          async def test_startup():
              config = Config({'server': {'debug': True}})
              server = ICELocatorServer(config)
              try:
                  await server.initialize()
                  print('✅ Server startup successful on ${{ matrix.os }}')
              except Exception as e:
                  print(f'❌ Server startup failed: {e}')
                  raise
              finally:
                  await server.cleanup()
          
          asyncio.run(test_startup())
          "

  documentation-tests:
    name: Documentation and Examples Tests
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mkdocs mkdocs-material
          pip install -e .
          
      - name: Test documentation build
        run: |
          mkdocs build --strict
          
      - name: Test code examples in documentation
        run: |
          # Extract and test code examples from documentation
          python -c "
          import os
          import re
          from pathlib import Path
          
          docs_dir = Path('docs')
          
          # Find all Python code blocks in markdown files
          for md_file in docs_dir.glob('**/*.md'):
              with open(md_file, 'r') as f:
                  content = f.read()
              
              # Extract Python code blocks
              python_blocks = re.findall(r'```python\n(.*?)\n```', content, re.DOTALL)
              
              for i, block in enumerate(python_blocks):
                  print(f'Found Python code block in {md_file}')
                  # Basic syntax check
                  try:
                      compile(block, f'{md_file}:block{i}', 'exec')
                  except SyntaxError as e:
                      print(f'Syntax error in {md_file} block {i}: {e}')
          "

  final-validation:
    name: Final Integration Validation
    runs-on: ubuntu-latest
    needs: [
      integration-tests-advanced,
      performance-tests,
      client-compatibility-tests,
      cross-platform-tests
    ]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all test results
        uses: actions/download-artifact@v3
        
      - name: Consolidate test results
        run: |
          python -c "
          import json
          import glob
          from pathlib import Path
          
          # Collect all test reports
          all_results = []
          report_files = glob.glob('*/integration_test_report.json')
          
          for report_file in report_files:
              try:
                  with open(report_file, 'r') as f:
                      report = json.load(f)
                  all_results.append(report)
              except Exception as e:
                  print(f'Error reading {report_file}: {e}')
          
          # Consolidate summary
          total_summary = {
              'total_tests': 0,
              'passed': 0,
              'failed': 0,
              'errors': 0,
              'skipped': 0
          }
          
          all_test_results = []
          
          for result in all_results:
              if 'summary' in result:
                  for key in total_summary:
                      total_summary[key] += result['summary'].get(key, 0)
              
              if 'test_results' in result:
                  all_test_results.extend(result['test_results'])
          
          # Calculate final metrics
          success_rate = total_summary['passed'] / max(1, total_summary['total_tests'])
          error_rate = (total_summary['failed'] + total_summary['errors']) / max(1, total_summary['total_tests'])
          
          print('🏆 FINAL INTEGRATION TEST SUMMARY')
          print('=' * 50)
          print(f'Total Tests: {total_summary[\"total_tests\"]}')
          print(f'✅ Passed: {total_summary[\"passed\"]}')
          print(f'❌ Failed: {total_summary[\"failed\"]}')
          print(f'⚠️  Errors: {total_summary[\"errors\"]}')
          print(f'⏭️  Skipped: {total_summary[\"skipped\"]}')
          print(f'📈 Success Rate: {success_rate:.1%}')
          print(f'📉 Error Rate: {error_rate:.1%}')
          
          # Validation
          validation_passed = success_rate >= 0.9 and error_rate <= 0.1
          
          print(f'🎯 VALIDATION: {\"PASSED\" if validation_passed else \"FAILED\"}')
          
          # Save consolidated report
          consolidated_report = {
              'summary': total_summary,
              'success_rate': success_rate,
              'error_rate': error_rate,
              'validation_passed': validation_passed,
              'test_results': all_test_results
          }
          
          with open('consolidated_integration_report.json', 'w') as f:
              json.dump(consolidated_report, f, indent=2)
          
          # Exit with error if validation failed
          if not validation_passed:
              print('❌ Integration validation FAILED')
              exit(1)
          else:
              print('✅ Integration validation PASSED')
          "
          
      - name: Upload consolidated report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: consolidated-integration-report
          path: consolidated_integration_report.json
          
      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = JSON.parse(fs.readFileSync('consolidated_integration_report.json', 'utf8'));
              
              const comment = `## 🚀 Integration Test Results
              
              **Overall Status:** ${report.validation_passed ? '✅ PASSED' : '❌ FAILED'}
              
              ### Summary
              - **Total Tests:** ${report.summary.total_tests}
              - **Passed:** ${report.summary.passed} (${(report.success_rate * 100).toFixed(1)}%)
              - **Failed:** ${report.summary.failed}
              - **Errors:** ${report.summary.errors}
              - **Skipped:** ${report.summary.skipped}
              
              ### Validation Criteria
              - **Success Rate:** ${(report.success_rate * 100).toFixed(1)}% ${report.success_rate >= 0.9 ? '✅' : '❌'}
              - **Error Rate:** ${(report.error_rate * 100).toFixed(1)}% ${report.error_rate <= 0.1 ? '✅' : '❌'}
              
              ${!report.validation_passed ? '⚠️ **This PR does not meet integration test criteria and should not be merged.**' : ''}
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post comment:', error);
            }